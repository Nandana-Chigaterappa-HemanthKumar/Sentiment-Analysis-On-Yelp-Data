{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn torch transformers tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeL60IpL-MHv",
        "outputId": "3c937df2-9073-4791-a6b8-cb2924183970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\r\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.6.1)\r\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\r\n",
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.49.0)\r\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.2.2)\n",
            "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
            "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.31.6)\n",
            "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.29.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2023.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwJzlcwy-m4j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"processed_train.csv\")\n",
        "test_df = pd.read_csv(\"processed_test.csv\")"
      ],
      "metadata": {
        "id": "js4-uVo6--Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values in processed files\n",
        "print(\"Null values in train_df:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nNull values in test_df:\")\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwspGEL-_Asp",
        "outputId": "bfdc0af5-2076-4364-c8d1-e7154e5d1eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values in train_df:\n",
            "sentiment           0\n",
            "processed_review    0\n",
            "dtype: int64\n",
            "\n",
            "Null values in test_df:\n",
            "sentiment           0\n",
            "processed_review    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.info())\n",
        "print(test_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuQjL5Wi_Dpp",
        "outputId": "56f44c3b-3db2-4307-a146-64586fd74998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 519951 entries, 0 to 519950\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count   Dtype \n",
            "---  ------            --------------   ----- \n",
            " 0   sentiment         519951 non-null  int64 \n",
            " 1   processed_review  519951 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 7.9+ MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 39998 entries, 0 to 39997\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   sentiment         39998 non-null  int64 \n",
            " 1   processed_review  39998 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 625.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the train data into train and validation sets (90% Train, 10% Validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_df[\"processed_review\"], train_df[\"sentiment\"], test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "X_test = test_df[\"processed_review\"]\n",
        "y_test = test_df[\"sentiment\"]\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mvU9mM6_GFJ",
        "outputId": "8b77c859-24eb-42f0-c24c-5f90a320f93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 467955, Validation size: 51996, Test size: 39998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Checking if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrcdIGLnAdUM",
        "outputId": "f242f636-3a6f-4ed3-b0b2-4a02b7159163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.0 torchtext==0.15.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auGWh6c4CCVx",
        "outputId": "1fcc69c1-89c6-4fb9-85e8-c4068a6a5ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.0 in /opt/conda/lib/python3.10/site-packages (2.0.0)\r\n",
            "Requirement already satisfied: torchtext==0.15.1 in /opt/conda/lib/python3.10/site-packages (0.15.1)\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (4.7.1)\r\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (1.12)\r\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.1)\r\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.1.2)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.7.99)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.7.99)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.7.101)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (8.5.0.96)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.10.3.66)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (10.9.0.58)\r\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (10.2.10.91)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.4.0.1)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.7.4.91)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (2.14.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (11.7.91)\r\n",
            "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (2.0.0)\r\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.1) (4.65.0)\r\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.1) (2.31.0)\r\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.1) (1.26.2)\r\n",
            "Requirement already satisfied: torchdata==0.6.0 in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.1) (0.6.0)\r\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (68.2.2)\r\n",
            "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.41.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.6.0->torchtext==0.15.1) (1.26.18)\n",
            "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Loading the tokenizer\n",
        "text_tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Checking if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Tokenization Running on: {device}\")\n",
        "\n",
        "def yield_tokens(data):\n",
        "    for text in tqdm(data, desc=\"Building Vocabulary\", unit=\"sentence\"):\n",
        "        yield text_tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "def tokenize_texts(texts, vocab, tokenizer, max_len=256, batch_size=512):\n",
        "    \"\"\"Tokenizes text in batches and moves to GPU for efficiency.\"\"\"\n",
        "    all_encodings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\", unit=\"batch\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        encodings = [\n",
        "            [vocab[token] for token in tokenizer(text)] for text in batch_texts\n",
        "        ]\n",
        "\n",
        "        # Padding sequences\n",
        "        encodings = [\n",
        "            seq + [vocab[\"<pad>\"]] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "            for seq in encodings\n",
        "        ]\n",
        "\n",
        "        encodings = torch.tensor(encodings, dtype=torch.long).to(device)\n",
        "        all_encodings.append(encodings)\n",
        "\n",
        "    return torch.cat(all_encodings, dim=0)\n",
        "\n",
        "train_encodings = tokenize_texts(X_train, vocab, text_tokenizer)\n",
        "val_encodings = tokenize_texts(X_val, vocab, text_tokenizer)\n",
        "test_encodings = tokenize_texts(X_test, vocab, text_tokenizer)\n",
        "\n",
        "print(\"Tokenization completed and moved to GPU!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEDqtAmUAi3N",
        "outputId": "6f67a64f-24f4-46bd-87f3-df7609438163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization Running on: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Vocabulary: 100%|██████████| 467955/467955 [00:09<00:00, 48055.67sentence/s]\n",
            "Tokenizing: 100%|██████████| 914/914 [00:27<00:00, 33.51batch/s]\n",
            "Tokenizing: 100%|██████████| 102/102 [00:02<00:00, 35.48batch/s]\n",
            "Tokenizing: 100%|██████████| 79/79 [00:02<00:00, 37.87batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization completed and moved to GPU!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "class YelpDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch dataset for tokenized text data\"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings  # Already tokenized and padded sequences\n",
        "        self.labels = torch.tensor(labels.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encodings[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = YelpDataset(train_encodings, y_train)\n",
        "val_dataset = YelpDataset(val_encodings, y_val)\n",
        "test_dataset = YelpDataset(test_encodings, y_test)\n",
        "\n",
        "print(\"Datasets created and moved to GPU!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7eVoZVyA9F9",
        "outputId": "f17e5a7c-66e6-46db-f86e-1549b2e4543f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets created and moved to GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Creating DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=RandomSampler(train_dataset))\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=SequentialSampler(val_dataset))\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=SequentialSampler(test_dataset))\n",
        "\n",
        "print(\"DataLoaders ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyMca-58HJcs",
        "outputId": "4425cc6d-89cd-4e40-dec0-4a8b7e5e8d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Defining the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512, hidden_dim=512, num_layers=4, output_dim=2, dropout=0.3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 1024)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(1024, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # Bidirectional hidden state concatenation\n",
        "        out = self.fc1(hidden)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return self.softmax(out)\n",
        "\n",
        "# Initializing the Model\n",
        "VOCAB_SIZE = len(vocab)\n",
        "model = LSTMClassifier(VOCAB_SIZE, embed_dim=512, hidden_dim=512, num_layers=4, output_dim=2, dropout=0.3).to(device)\n",
        "\n",
        "print(f\"Model loaded on: {device}\")"
      ],
      "metadata": {
        "id": "U9aZSJtnHjA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bee0b9c-c6ba-498d-9ace-d61095f707ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "PATIENCE = 2\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Stops training if validation loss does not improve after `patience` epochs.\"\"\"\n",
        "    def __init__(self, patience=2):\n",
        "        self.patience = patience\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.counter = 0\n",
        "\n",
        "    def check_early_stop(self, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"\\n Early stopping triggered! Stopping training.\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "early_stopping = EarlyStopping(patience=PATIENCE)"
      ],
      "metadata": {
        "id": "nle36yk5ILJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=EPOCHS):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n Epoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        total_loss, total_correct = 0, 0\n",
        "        loop = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\", dynamic_ncols=True)\n",
        "\n",
        "        for texts, labels in loop:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_acc = total_correct / len(train_loader.dataset)\n",
        "        print(f\"\\n Training Loss: {avg_train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        total_correct, total_loss = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in val_loader:\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = total_loss / len(val_loader)\n",
        "        val_acc = total_correct / len(val_loader.dataset)\n",
        "        print(f\"\\n Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        # Checking for improvement and saving the best model's state\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0  # Resetting the patience counter\n",
        "            # Saving the best model's state dictionary in .pt format\n",
        "            torch.save(model.state_dict(), 'LSTM_Best_Model_State.pt')\n",
        "            print(\"Best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(\"\\nEarly stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "HXfVhvUeI8Fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41aef597-ac32-4066-c6d6-96e19dd34def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 14624/14624 [26:14<00:00,  9.29batch/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Loss: 0.2825, Accuracy: 0.8805\n",
            "\n",
            " Validation Loss: 0.2285, Accuracy: 0.9046\n",
            "Best model saved!\n",
            "\n",
            " Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 14624/14624 [26:01<00:00,  9.36batch/s, loss=0.224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Loss: 0.2101, Accuracy: 0.9153\n",
            "\n",
            " Validation Loss: 0.1998, Accuracy: 0.9191\n",
            "Best model saved!\n",
            "\n",
            " Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 14624/14624 [26:29<00:00,  9.20batch/s, loss=0.222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Loss: 0.1828, Accuracy: 0.9270\n",
            "\n",
            " Validation Loss: 0.1883, Accuracy: 0.9253\n",
            "Best model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Seting the model to evaluation mode\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    loop = tqdm(test_loader, desc=\"Evaluating Test Set\", unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loop:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Converting to NumPy\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    return all_preds, all_labels"
      ],
      "metadata": {
        "id": "baYi4RLGQFbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('LSTM_Best_Model_State.pt'))\n",
        "test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "# Computing performance metrics\n",
        "accuracy = accuracy_score(test_labels, test_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average=\"binary\")\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "wZECPG_2O_Gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fec575-68e7-4e1c-c3ff-30121fb9585f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Set: 100%|██████████| 1250/1250 [00:44<00:00, 28.34batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.9252\n",
            "Precision: 0.9270\n",
            "Recall: 0.9233\n",
            "F1-Score: 0.9251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZXUu7dBPMuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}